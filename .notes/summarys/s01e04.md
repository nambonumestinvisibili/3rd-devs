# S01E04 – Techniki optymalizacji: Podsumowanie i kompendium

## Najważniejsze informacje i koncepty

- **Optymalizacja promptów i kodu**  
  Optymalizacja kodu i promptów jest kluczowa dla skuteczności aplikacji z LLM. Precyzja instrukcji i logiki wpływa bezpośrednio na jakość wyników.

- **Iteracyjne podejście do promptów**  
  Prompty powinny być rozwijane iteracyjnie, z wykorzystaniem narzędzi takich jak LangFuse i PromptFoo do monitorowania, debugowania i ewaluacji.

- **Meta prompt**  
  Meta prompt to specjalny szablon/systemowa instrukcja, która pomaga tworzyć i optymalizować inne prompty, prowadząc użytkownika przez proces projektowania instrukcji.

- **Techniki projektowania promptów**  
  Warto znać i stosować techniki takie jak Few-Shot, Chain of Thought (CoT), Tree of Thoughts, SMART, czy meta prompt engineering.

- **Współpraca z LLM**  
  LLM mogą wspierać proces optymalizacji promptów, generować przykłady, testy i pomagać w iteracji, ale kluczowa rola pozostaje po stronie człowieka.

- **Embedding i vector store**  
  Wyszukiwanie semantyczne opiera się na embeddingach i bazach wektorowych (np. FAISS). Pozwala to na lepsze dopasowanie informacji do kontekstu rozmowy.

 Sam temat baz wektorowych można zrozumieć znacznie lepiej, gdy nazwiemy je po prostu silnikami wyszukiwania, podobnymi do ElasticSearch czy Algolia. Jednak tutaj zamiast szukać dopasowanych sposobem zapisu, szukamy fraz podobnych pod kątem znaczenia. Np. podobny zapis to król, królowa a podobne znaczenie to król i mężczyzna oraz królowa i kobieta.
 Znaczenie słów opisuje się z pomocą modeli generujących tzw. embedding, czyli zestaw liczb (wektorów) reprezentujących różne cechy treści. W naszych przykładach korzystamy z modelu text-embedding-3-large od OpenAI, jednak już teraz jest istnieją znacznie lepsze modele, wymienione na MTEB Leaderboard. W zależności od zastosowania (np. pracy z językiem polskim) mogą sprawdzić się inne modele. Co więcej, w przypadku nowych słów (np. projektu Tech•sistence), model może mieć trudność w opisaniu ich znaczenia i trzeba o tym pamiętać oraz stosować wyszukiwanie hybrydowe.
- **Debugowanie i analiza promptów**  
  Analiza logów, zadawanie pogłębiających pytań modelowi oraz rozbijanie zadań na mniejsze kroki pomagają w identyfikacji i eliminacji błędów.


- Skorzystałem więc z odpowiednika OpenAI Playground dostępnego w LangFuse, aby wysłać kolejną wiadomość oznaczoną jako "System Check" z prośbą o wyjaśnienie decyzji. W odpowiedzi otrzymałem uzasadnienie, w którym model uznał kategorię notepad za bardziej pasującą do wskazanego zasobu. Oczywiście, ze względu na sposób działania LLM, nie mówimy tutaj o 100% pewności, że wskazany przez model powód jest tym, którego szukamy, ale możemy uznać go za wskazówkę.
- Zadać pytania pogłębiające, które mogą naprowadzić nas na kolejne problemy promptu lub nowe pomysły dotyczące jego dalszej iteracji.
- Przeanalizować prompt samodzielnie, szczególnie pod kątem brakujących danych, dwuznacznych instrukcji czy nawet kolejności zapisu poszczególnych sekcji.
- Rozbić zadanie na mniejsze kroki, uwzględniając np. wcześniejsze zastanowienie się nad organizacją wspomnień.
- Dać więcej czasu "na myślenie" przez generowanie właściwości "thinking", umieszczanej na pierwszym miejscu struktury obiektu JSON.
- Wprowadzić zmiany wspólnie z modelem.

- **Kompresja promptów**  
  Skracanie i upraszczanie promptów (np. z pomocą LLMLingua) pozwala na oszczędność tokenów i lepsze zarządzanie uwagą modelu.

Moje doświadczenie z tym narzędziem sugeruje, że automatyczna kompresja promptu daje umiarkowane rezultaty, ale stanowi dobre źródło informacji na temat tego, co potencjalnie możemy usunąć. Tutaj także mamy przestrzeń do tworzenia meta promptów zdolnych do skutecznej kompresji.
  Kompresja promptów wydaje się mieć umiarkowane znaczenie, biorąc pod uwagę rosnące limity okna kontekstu czy cache'owanie promptu. Natomiast już teraz sami widzimy, że złożone modele mają problem z podążaniem za złożonymi instrukcjami, które także szybko stają się mało zrozumiałe także dla ludzi.

W temacie kompresji, pod uwagę możemy brać:





Zastępowanie obszernych, złożonych promptów, na mniejsze, bardziej wyspecjalizowane akcje, które będą uruchamiane warunkowo, w zależności od sytuacji.



Opisywanie wybranych zachowań modelu z pomocą pojedynczych słów bądź wyrażeń, a nie pełnych zdań. Przykładowo określenie "Use first-principles thinking" wskazuje na zrozumiałą dla modelu językowego koncepcję rozumowania, poprzez rozbicie zagadnienia na czynniki pierwsze.



Zmianę języka promptu oraz kontekstu, np. z polskiego na angielski



Usuwanie wybranych fragmentów promptu, które opisują naturalne zachowanie modelu (np. sposób formatowania wypowiedzi) i tym samym nie wnoszą nic nowego. Co więcej, takie instrukcje mogą ograniczać model, zmniejszając jego skuteczność
- **Optymalizacja wypowiedzi modelu**  
  Warto dążyć do krótkich, precyzyjnych odpowiedzi, stosować właściwości typu `_thinking` i jasno określać strukturę odpowiedzi.

  Koszt liczby wygenerowanych tokenów jest większy niż koszt tokenów przesłanych do modelu. Oczywiście tych drugich niemal zawsze jest zdecydowanie więcej, jednak i tak większy wpływ na ceny oraz szybkość działania aplikacji mają tokeny wyjściowe.

  Dobrym przykładem jest właściwość _thinking, która pojawia się w moich promptach jako pierwsza właściwość generowanego obiektu JSON. W ten sposób stwarzam modelowi przestrzeń na zastanowienie się nad tym, jak mają wyglądać kolejne właściwości zwracanego obiektu.

  Jeśli nasza instrukcja w tej sytuacji będzie sugerować modelowi wyłącznie "zastanowienie się", to z dużym prawdopodobieństwem otrzymamy rozbudowaną treść o wątpliwej wartości. Jednak gdy wskażemy schemat myśli lub listę pytań, na które model musi odpowiedzieć, jakość wypowiedzi znacznie wzrośnie.

- **Fine-tuning**  
  Dostosowanie modelu do specyficznych zadań przez fine-tuning uzupełnia optymalizację promptów, ale nie zastępuje jej.

## Technologie i narzędzia

- **LangFuse** – monitoring, debugowanie i analiza promptów.
- **PromptFoo** – ewaluacja i testowanie promptów.
- **FAISS** – baza wektorowa do przechowywania embeddingów.
- **LLMLingua** – narzędzie do kompresji promptów.
- **OpenAI, HuggingFace** – modele embeddingów i LLM.
- **Meta prompt** – szablon/systemowa instrukcja do optymalizacji promptów.

## Tipy i dobre praktyki

- Rozwijaj prompty iteracyjnie, testuj i analizuj wyniki.
- Korzystaj z narzędzi do monitoringu i ewaluacji.
- Stosuj techniki Few-Shot, CoT, meta prompt engineering.
- Kompresuj prompty i wypowiedzi, by oszczędzać tokeny.
- Współpracuj z modelem, ale zachowuj kontrolę nad procesem.
- Ustal jasną strukturę odpowiedzi i przykłady Few-Shot.
- Rozważ fine-tuning dla wyspecjalizowanych zadań.

## Ciekawostki

- Modele LLM mają już wiedzę o technikach prompt engineeringu i potrafią je stosować.
- Automatyczna kompresja promptów daje umiarkowane rezultaty, ale jest dobrym punktem wyjścia do ręcznej optymalizacji.
- Nawet niewielkie zmiany w strukturze promptu mogą znacząco wpłynąć na wyniki modelu.
- Modele mogą generować własne uzasadnienia decyzji, co pomaga w debugowaniu.

## Linki i odniesienia

- [Large Language Models as Optimizers](https://arxiv.org/abs/2309.03409)
- [Large Language Models Are Human-Level Prompt Engineers](https://arxiv.org/abs/2309.03409)
- [Lost in The Middle](https://arxiv.org/pdf/2307.03172)
- [A Challenge to Long-Context LLMs and RAG Systems](https://arxiv.org/pdf/2407.01370)
- [LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models](https://arxiv.org/abs/2310.05736)
- [LLMLingua GitHub](https://github.com/microsoft/LLMLingua)
- [A Survey of Techniques for Maximizing LLM Performance (YouTube)](https://www.youtube.com/watch?v=ahnGLM-RC1Y)
- [MTEB Leaderboard – HuggingFace](https://huggingface.co/spaces/mteb/leaderboard)
- [Meta prompt – przykładowy szablon](https://cloud.overment.com/AI_devs-3-Prompt-Engineer-1726336422-1726339115.md)

---

To kompendium stanowi praktyczne podsumowanie lekcji o technikach optymalizacji w pracy z dużymi modelami językowymi.