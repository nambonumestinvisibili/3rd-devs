Pseudocode plan:
1. Przeczytaj cały plik i zidentyfikuj kluczowe sekcje tematyczne.
2. Dla każdej sekcji wypisz najważniejsze informacje, koncepty, technologie i tipy.
3. Każdy punkt opisz krótko, zwięźle i konkretnie.
4. Zwróć listę punktów jako podsumowanie.

Markdown summary:

- **Sterowanie kontekstem modelu**  
  Możliwość rozszerzania wiedzy LLM przez dostarczanie zewnętrznych danych do promptu (ręcznie lub automatycznie, np. RAG).

- **Źródła danych i ich wyzwania**  
  Kontekst może pochodzić z baz danych, API, plików – wymaga to transformacji, synchronizacji i przemyślenia struktury.

- **Formaty wymiany danych**  
  Preferowane są otwarte formaty: markdown, txt, json, yaml. Pliki binarne (PDF, DOCX) wymagają konwersji i mogą prowadzić do utraty informacji. 

- **Transformacja i kompresja treści**  
  Przetwarzanie danych do form przyjaznych LLM (np. HTML → markdown). YAML może być bardziej efektywny niż JSON pod względem tokenów. Chunking (dzielenie na fragmenty) i generowanie notatek zamiast prostego dzielenia. Dla prostego obiektu JSON mówimy o 30% różnicy tokenów (według Tiktokenizer i modelu GPT-4o), których model nie musi generować, jeśli zapiszemy te dane w formacie YAML. To przekłada się także na niższe koszty oraz krótszy czas inferencji. Zatem pracując z różnymi formatami danych, zawsze w pierwszej kolejności warto zadać sobie pytanie o to, czy możemy dokonać transformacji do bardziej przyjaznej formy. Tu są pieniądze i rozproszenie modelu.

- **Podsumowanie treści**
  W zamian możemy przeprowadzić bardziej zaawansowane przetwarzanie treści pliku, analizując go w całości, często kilkukrotnie, aby wygenerować nowe dane. Mowa tutaj o utworzeniu notatek na temat omawianych koncepcji, definicji czy problemów. W rezultacie, zamiast zestawiać zapytanie użytkownika z oryginalną treścią pliku, rolę kontekstu przejmują wygenerowane notatki. Przykład takiego podejścia widać poniżej, gdzie z wgranego pliku zostaje wygenerowane ogólne podsumowanie i lista koncepcji, które stanowią kontekst zapytania.

Jeśli na etapie przetwarzania pliku nie popełnimy dużych błędów, to ryzyko pominięcia istotnych danych, jest mniejsze, niż w przypadku dzielenia dokumentu na fragmenty.

- **Dedykowane źródła wiedzy**  
  Można budować własne bazy wiedzy dla LLM (np. projekt mem0, files), często w formie plików markdown i z użyciem vector store (faiss, Qdrant).

- **Synchronizacja i identyfikatory**  
  Przy integracji z zewnętrznymi źródłami (np. blogi) ważne jest zachowanie identyfikatorów i powiązań między fragmentami a oryginałem.

- **Dostarczanie kontekstu do modelu**  
  API modeli jest bezstanowe – trzeba przekazywać cały kontekst lub stosować cache/powtórne wyszukiwanie. Kontekst powinien być dynamicznie dobierany.

- **Optymalizacja przepływu danych**  
  Przetwarzaj tylko niezbędne dane, kontroluj przepływ informacji, planuj działania modelu i reaguj na nieprzewidziane sytuacje.

- **Tipy praktyczne**  
  - Pracuj na otwartych formatach.
  - Transformuj dane do form przyjaznych LLM.
  - Buduj własne bazy wiedzy, jeśli to możliwe.
  - Zawsze zachowuj powiązania z oryginałem.
  - Optymalizuj liczbę tokenów (np. YAML zamiast JSON).
  - Przemyśl synchronizację i aktualizację danych.
  - Testuj rozwiązania na własnych workflow (np. automatyczne przypisywanie zadań).

- **Technologie i narzędzia**  
  - Markdown, JSON, YAML
  - Notion-to-md
  - Vector store: faiss, Qdrant
  - Obsidian (wizualizacja wiedzy)
  - Webhooki, harmonogramy synchronizacji
  - Przykłady: linear, files, websearch, mem0

- **Wnioski**  
  Dostarczanie własnego kontekstu pozwala optymalizować zadania, zwiększać skuteczność i elastyczność aplikacji generatywnych.