Oto podsumowanie, kluczowe punkty, użyte linki i nowe koncepcje z przedstawionej lekcji AI\_devs 3:

---

## AI_devs 3: Interakcja z dużymi modelami językowymi (LLM) – Podsumowanie i Kluczowe Wnioski

Lekcja wprowadza w koncepcję **programistycznej interakcji z dużymi modelami językowymi (LLM) poprzez API**, koncentrując się na budowaniu **częściowo autonomicznych narzędzi zwanych "Agentami AI"**. Podkreśla, że mimo nowatorskich możliwości LLM, ich integracja z aplikacjami wymaga solidnych podstaw programistycznych, a ~80% kodu Agenta AI to klasyczna aplikacja.

---

### Kluczowe Punkty i Koncepcje

* **Agenci AI jako narzędzia programistyczne:** LLM-y to narzędzia do przetwarzania i generowania danych w sposób niemożliwy wcześniej (np. regexem). Agenci AI to złożone rozwiązania, składające się z indywidualnych komponentów i modułów, które można łączyć.
* **Rozwój ekosystemu LLM:** Rynek LLM szybko się rozwija, oferując wiele modeli od różnych dostawców (OpenAI, Anthropic, Google Vertex AI, xAI, Amazon Bedrock, Azure, Groq itp.). To daje możliwość wyboru pod kątem cen, limitów API, polityki prywatności i możliwości samych modeli.
* **Koszty i wydajność:** Interakcja z LLM, zwłaszcza w przypadku agentów działających autonomicznie na dużych zbiorach danych (miliony tokenów), generuje zauważalne koszty i może mieć duży czas reakcji (np. 18 tys. tokenów i 24 sekundy na zapytanie).
* **Automatyzacja procesów:**
    * **Klasyczna automatyzacja:** Oparta na ściśle zdefiniowanych regułach programistycznych (np. monitorowanie e-maili i słów kluczowych). Wymaga precyzyjnych reguł.
    * **Automatyzacja z LLM:** Umożliwia swobodne interpretowanie różnych formatów treści z wielu źródeł (wiadomości, zdjęcia, nagrania głosowe, inne API/agenci AI) i dynamiczne dostosowywanie się do sytuacji, dzięki interpretacji modelu.
* **4 Etapy działania Agenta AI (cykl interakcji):**
    1.  **Zrozumienie:** Wczytywanie pamięci, dostęp do Internetu, analiza.
    2.  **Plan działań:** Generowanie listy akcji na podstawie "przemyśleń" i dostępnych narzędzi/umiejętności.
    3.  **Podejmowanie działań:** Wykonywanie kroków, zbieranie informacji zwrotnych.
    4.  **Odpowiedź:** Generowanie ostatecznej odpowiedzi na podstawie wiedzy i raportu z działań.
* **Sterowanie konwersacją (Thread Example):**
    * Zamiast przesyłać całą historię konwersacji, stosuje się **podsumowanie** poprzedniej rozmowy oraz najnowszą wiadomość użytkownika.
    * **Zalety podsumowania:** Optymalizacja kosztów (tańszy model do podsumowania), lepsza koncentracja modelu (mniejszy kontekst), unikanie limitu okna kontekstu, możliwość wykorzystania podsumowania w innych częściach logiki agenta.
    * **Straty:** Naturalnie tracimy część informacji, ale można dodać mechanizm przeszukiwania wcześniejszych wątków.
* **Rodzaje interakcji / Elementarne akcje LLM:** Agenci wykonują złożone zadania poprzez moduły i pojedyncze akcje, takie jak:
    * **Podejmowanie decyzji:** Porównywalne do `if`/`switch`, ale bardziej elastyczne (kosztem determinizmu). Przykład: `use search` (klasyfikacja 0/1 do użycia wyszukiwarki).
    * **Klasyfikacja:** Wybór wielu opcji (np. `pick_domains` do wyboru domen wyszukiwania).
    * **Parsowanie, Transformacja, Ocena:** Model może oceniać istotność zwróconych wyników (np. `rate` dla wyników wyszukiwania).
* **Thought Generation / Chain of Thought:** Technika zwiększająca skuteczność LLM, dająca modelowi "czas na myślenie" (np. `_thoughts` w `pick_domains`).
* **Przykłady (`Few-Shot` / `Many-Shot`):**
    * Pierwsza technika optymalizacji skuteczności promptu, wzmacniająca główną instrukcję.
    * Zwykle 3-40 par wejście-wyjście. Powinny być zróżnicowane, uwzględniać sytuacje brzegowe. Mogą być generowane przez modele i wykorzystywane do Fine-Tuning.
* **Retrieval-Augmented Generation (RAG):**
    * Wczytywanie zewnętrznych treści (np. z wyszukiwarki, stron www za pomocą FireCrawl, baz wiedzy) do promptu, aby rozszerzyć bazową wiedzę modelu.
    * **Wyzwania RAG:** Problemy z formatowaniem danych, ograniczona wiedza LLM na temat kontekstu (konieczność parafrazy wspomnień, np. "overment to mój nickname" powinno być zapamiętane jako "Nickname Adama to overment").
    * **Zasady korzystania z kontekstu:** Instrukcja powinna mówić, jak model ma wykorzystywać kontekst, wyraźne oddzielanie wielu kontekstów, instrukcja zachowania przy niewystarczającym kontekście.
* **Architektura aplikacji z LLM:**
    * **~80% klasycznej aplikacji:** Większość kodu to standardowe programowanie.
    * **Przepływ danych:** Działanie kolejnych promptów jest uzależnione od rezultatów poprzednich.
    * **Dodatkowe komponenty/zagadnienia:**
        * **Baza danych (np. PostgreSQL):** Do przechowywania historii konwersacji, wyników wyszukiwania i kontekstu.
        * **Silnik wyszukiwania (np. Qdrant):** Do efektywnego odszukiwania danych (wektoryzacja).
        * **Zarządzanie stanem:** Historia promptów, uruchomionych narzędzi i feedbacku.
        * **API:** Budowa narzędzi, którymi LLM może się posługiwać (rozumienie odpowiedzi, obsługa błędów).
        * **Ewaluacja promptów (PromptFoo):** Automatyczne testowanie promptów na zestawach testowych.
        * **Wersjonowanie i kopie zapasowe:** Kodu, promptów i zmian wprowadzanych przez model.
        * **Kontrola uprawnień:** Ograniczanie dostępu LLM do domen/zasobów.
        * **Monitorowanie aplikacji:** Zaawansowane narzędzia (np. LangFuse) zamiast plików markdown.
        * **Asynchroniczność:** Uruchamianie działań LLM w tle, kolejki, powiadomienia.
        * **Interfejs:** Dostosowanie do użytkownika (czat, formularze, harmonogramy).

---

### Użyte Narzędzia i Technologie

* **LLM (modele językowe):** OpenAI (o1, GPT, TTS, Whisper, Embedding), Anthropic (Claude), Google Vertex AI (Gemini), xAI (Grok), Amazon Bedrock, Azure (OpenAI, Meta), Groq (Open Source, Llama).
* **Narzędzia:**
    * **PromptFoo:** Narzędzie do automatycznego testowania i ewaluacji promptów.
    * **FireCrawl:** Narzędzie do wczytywania treści ze stron internetowych.
    * **Linear, Todoist, ClickUp:** Przykłady aplikacji do zarządzania zadaniami.
    * **PostgreSQL, Qdrant:** Przykłady baz danych i silników wyszukiwania (wektorowych).
    * **LangFuse:** Narzędzie do zaawansowanego monitorowania aplikacji z LLM.
    * **Bun:** Środowisko uruchomieniowe (użyte w przykładzie `bun thread`).
    * **Cursor IDE:** IDE wspierające pracę z dokumentacją i promptami.

---

### Nowe Koncepty / Terminy Wprowadzone

* **Agenci AI:** Częściowo autonomiczne narzędzia programistyczne wykorzystujące LLM.
* **ChatML:** Format konwersacji (tablica messages + instrukcja systemowa).
* **Podsumowanie konwersacji (Summarization):** Technika oszczędzająca tokeny i poprawiająca skupienie modelu.
* **Few-Shot Learning / Many-Shot In-Context Learning:** Technika optymalizacji promptów poprzez dostarczanie przykładów wejścia-wyjścia.
* **Thought Generation / Zero-Shot Chain of Thought:** Technika zwiększająca skuteczność LLM poprzez generowanie "przemyśleń" wewnętrznych.
* **Retrieval-Augmented Generation (RAG):** Rozszerzanie wiedzy modelu o dane zewnętrzne (np. z wyszukiwarki, baz wiedzy).
* **Self-Querying:** Generowanie dodatkowych zapytań do silnika wyszukiwania na podstawie oryginalnego zapytania.
* **Re-rank:** Ocena i filtrowanie zwróconych wyników.
* **Architektura Kognitywna:** Podział aplikacji na komponenty takie jak Code, LLM Call, Chain, Router, State Machine.

---

### Zalecane Działania po Lekcji

* Uruchomienie przykładu **`websearch`** (`bun thread`, POST na `localhost:3000/api/demo`).
* Zadawanie pytań w `websearch` i analiza, dlaczego model może nie odpowiadać skutecznie.
* Przejrzenie promptów w pliku `prompts.ts` z przykładu `websearch`.
* Sprawdzenie skuteczności **FireCrawl** w wczytywaniu treści stron.
* Praca z narzędziem **PromptFoo** (konfiguracja z lekcji S00E02).

---

Lekcja podkreśla, że choć LLM otwierają nowe możliwości, **fundamentem tworzenia aplikacji nadal pozostaje wiedza i doświadczenie programistyczne**.




---

O PROMPTACH:
Jakość wypowiedzi modelu zależy od promptu, ale także od dostarczonych danych
Instrukcja powinna zawierać informacje na temat tego, jak model powinien wykorzystywać kontekst w swoich wypowiedziach
Jeden prompt może zawierać wiele zewnętrznych kontekstów, jednak powinny być one wyraźnie od siebie oddzielone
Model powinien posiadać instrukcję u zachowaniu w sytuacji, gdy dostarczony kontekst jest niewystarczający do udzielenia odpowiedzi
Musimy zadbać nie tylko o jakość źródeł dostarczanych informacji, ale także o sposób ich przechowania i dostarczenia do modelu. Wspomniana wyżej parafraza wspomnienia pokazuje, że zawsze musimy zadawać sobie pytanie: Jak model będzie wykorzystywać dostarczoną wiedzę?


---

DO Zrobienia:
Po dzisiejszej lekcji spróbuj przynajmniej uruchomić przykład websearch i zadać mu kilka pytań w celu sprawdzenia jak się w nich odnajduje. Istnieje dość duże prawdopodobieństwo, że nie odpowie skutecznie na Twoje pytania — zastanów się wtedy dlaczego tak się dzieje. Przejdź przez prompty z pliku prompts.ts oraz sprawdź jak skutecznie FireCrawl radzi sobie z wczytywaniem treści stron, z którymi chcesz pracować.

Możesz także poświęcić chwilę na pracę z PromptFoo, którego podstawową konfigurację omawiałem w materiale wdrożeniowym i lekcji S00E02 — Prompt Engineering. Do pracy z tym narzędziem wykorzystaj Cursor IDE z wczytaną dokumentacją, co ułatwi generowanie plików konfiguracyjnych oraz szybsze zrozumienie tego rozwiązania.

Zamykając klamrą dzisiejszą lekcję, to już na tym etapie powinno być dla Ciebie zrozumiałe to, że LLM w kodzie aplikacji pozwala na sprawne przetwarzanie języka naturalnego, a także różnych formatów danych (np. audio czy obrazu). Daje to nam nowe możliwości, ale nadal fundamentem rozwoju aplikacji pozostaje Twoja wiedza oraz doświadczenie.