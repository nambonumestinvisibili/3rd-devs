# S01E03 – Limity: Podsumowanie najważniejszych informacji

## Kluczowe informacje i koncepty

- **Ograniczenia modeli generatywnych AI**  
  Modele nie zawsze rozumieją treść – często powielają schematy z danych treningowych. Przykłady pokazują, że nie są w stanie logicznie rozwiązywać prostych zagadek czy poprawnie implementować algorytmów.

- **Architektura modeli (Transformer, Mixture of Experts)**  
  Mechanizm uwagi (attention) pozwala modelom utrzymywać kontekst, ale nie jest to "ludzka inteligencja". Nowoczesne modele mogą korzystać z architektury Mixture of Experts, co wpływa na skuteczność w zależności od zadania.

- **Limity i koszty**  
  Modele mają limity tokenów, zapytań i kosztów. Kontrola wydatków i monitorowanie zużycia tokenów są kluczowe, szczególnie w środowisku produkcyjnym.

- **Monitorowanie i narzędzia**  
  Narzędzia takie jak LangFuse, LangSmith, Portkey, Parea umożliwiają monitorowanie zapytań, tokenów i kosztów. Integracja z aplikacją pozwala na lepsze debugowanie i optymalizację.

- **Liczenie tokenów**  
  Ważne jest programistyczne liczenie tokenów (np. z użyciem Microsoft Tokenizer lub Tiktokenizer) dla poprawnego zarządzania limitem okna kontekstu i kosztami.

- **Limity generowanych treści**  
  Ograniczenia długości odpowiedzi modelu wymagają dzielenia tekstu na fragmenty lub programistycznego wykrywania końca wypowiedzi i kontynuowania generowania.

Modele typu "reasoner" posiadają znacznie większy limit tokenów wyjściowych, lecz większość z nich używana jest właśnie na etapie "zastanawiania się". Wystarczy zatem sytuacja w której będziemy chcieli wprowadzić korektę, tłumaczenie czy dowolną inną transformację na tekście, który swoją objętością przekracza limit modelu i problem limitu tokenów wyjściowych stanie się dla nas istotny.

W takiej sytuacji mamy kilka możliwości:

- podzielić długi tekst na mniejsze fragmenty, z których każdy będzie krótszy niż dopuszczalny limit wypowiedzi modelu
- programistycznie wykrywać powód zakończenia wypowiedzi modelu i poprosić o kontynuację

Jeśli zatem wyślemy zapytanie z wiadomością "Write ten sentences about apples and put them in order", to domyślnie zadanie to nie zostanie wykonane poprawnie i zakończy się informacją o przekroczeniu wartości max_tokens, co widać poniżej.

Możemy więc programistycznie wykryć ten powód i automatycznie kontynuować konwersację, dołączając do niej prośbę o dalszą wypowiedź, zaczynającą się od znaku kończącego ostatnią wiadomość.

W przykładzie max_tokens warto także zwrócić uwagę na plik app.ts, gdzie znajduje się logika sprawdzająca, czy suma tokenów promptu oraz wypowiedzi modelu nie przekracza limitu kontekstu okna. Takie zapytanie i tak skończyłoby się błędem ze strony API, natomiast warto pamiętać o tym, aby precyzyjnie liczyć prompty i brać pod uwagę limity tokenów dla modelu, z którym pracujemy.

- **Własne ograniczenia i moderacja**  
  Można narzucać własne ograniczenia (np. przez Moderation API lub własne prompty oceniające). Jednak pełna kontrola nie jest możliwa – prompt injection i jailbreaking pozostają zagrożeniem.

Bardziej elastyczną strategią narzucania własnych ograniczeń, jest wprowadzenie dodatkowych promptów oceniających i/lub weryfikujących, których zadanie będzie skupiać się wyłącznie na ocenie zapytania użytkownika i/lub wypowiedzi modelu, pod kątem naszych własnych zasad. Co ciekawe, wprowadzając własną skalę ocen, możemy programistycznie blokować zapytania, które spróbują nadpisać logikę naszego promptu.

- **Techniki wzmacniania rozumowania**  
  Warto stosować prompty z miejscem na "myślenie" (np. tagi `<thinking>`), co zwiększa szansę na poprawne rozumowanie modelu.

Nim przejdziemy dalej, dodam, że w prompcie oceniającym, bardzo wskazane jest dodanie przestrzeni na "zastanowienie się". Możemy to zrobić albo poprzez oczekiwanie formatu JSON, albo poprzez format widoczny poniżej. Polega on na zastosowaniu tagów <thinking> oraz <result>, w których model może wpisać oczekiwaną treść, a następnie z pomocą wyrażenia regularnego możemy pobrać rezultat.
W bloku <thinking> model generując uzasadnienie, stopniowo zwiększa prawdopodobieństwo tego, że kolejne tokeny będą wygenerowane zgodnie z naszymi zasadami. Jest to jedna z najlepszych technik wzmacniania rozumowania modelu, szczególnie gdy połączymy ją z oceną rezultatu. Trzeba tylko zadbać o to, aby "pokazać modelowi jak ma myśleć", czyli przedstawić kilka przykładów zawartości bloku "thinking". W przeciwnym razie zwykle wygeneruje tam mało wartościową treść. 

- **Wydajność i optymalizacja**  
  Wydajność modeli rośnie, ale limity zapytań i tokenów nadal są wyzwaniem. Warto rozważyć równoległe zapytania, cache'owanie promptów, wybór mniejszych modeli lub przeniesienie części logiki do kodu.


W celu optymalizacji czasu realizacji wszystkie klasyfikacje zostały uruchomione równolegle. Takie podejście faktycznie zwiększa wydajność aplikacji, lecz naraża nas na przekroczenie limitów zapytań oraz limitu przetworzonych tokenów w czasie.

- Jak możemy zaprojektować logikę, aby realizować jak najwięcej zapytań równolegle?
- Czy możemy skorzystać z mniejszego, szybszego modelu, nawet kosztem bardziej obszernych promptów?
- Czy możemy skorzystać z mechanizmu cache'owania promptu w celu zmniejszenia czasu reakcji (np. w przypadku modeli Anthropic)?
- Czy możemy skorzystać z platform oferujących szybką inferencję?
- Czy w ogóle będzie zależało nam na wydajności, bo np. część z zadań może być realizowana w tle?
- Czy wszystkie z zadań musi realizować model i czy możemy przynajmniej część logiki, przenieść na kod (np. przez wyrażenia regularne)?

- **Modele niecenzurowane**  
  Istnieją modele open-source bez cenzury (np. Dolphin, Grok), które mogą generować treści niedostępne w modelach komercyjnych. Wymagają jednak odpowiedzialnego użycia.

## Najważniejsze tipy i praktyki

- Monitoruj zużycie tokenów i koszty od początku projektu.
- Ustaw limity dla użytkowników i aplikacji, by uniknąć niekontrolowanych wydatków.
- Stosuj narzędzia do monitoringu (np. LangFuse) i licz tokeny programistycznie.
- Dziel długie teksty na fragmenty, by nie przekroczyć limitów modeli.
- Moderuj treści zarówno wejściowe, jak i wyjściowe – najlepiej wielowarstwowo.
- Optymalizuj wydajność przez równoległość, cache i wybór odpowiednich modeli.
- Rozważaj użycie modeli niecenzurowanych tylko tam, gdzie to konieczne i bezpieczne.
- Nie wszystkie zadania wymagają użycia LLM – czasem lepiej użyć klasycznego kodu.

---



Monitorowanie aplikacji, przetworzonych tokenów oraz kosztów jest krytyczne zarówno z technologicznego, jak i biznesowego punktu widzenia



Kontrolowanie liczby tokenów dla przetwarzanej treści, a także limitu zapytań, również pozwoli nam uniknąć niepotrzebnych kosztów. Tutaj mowa o korzystaniu z tokenizera z ustawieniami dla aktualnego modelu



Limity platform (szybkość, rate limit, czas reakcji, stabilność) stanowią ogromny problem na produkcji. I choć sytuacja poprawia się z miesiąca na miesiąc, należy już na początkowym etapie uwzględnić ją w swoim planie



Moderacja treści trafiających do modelu oraz treści generowanych przez model, to proces który nie zapewnia 100% bezpieczeństwa i przewidywalności, lecz znacząco poprawia jakość działania aplikacji



Optymalizacja wydajności aplikacji wiąże się ze zmianami projektowymi, dzięki którym zapytania do modelu będą wykonywane równolegle, bądź w tle.



Nie wszystkie zadania musimy realizować z pomocą najlepszego modelu



Nie wszystkie zadania wymagają zaangażowania jakiegokolwiek modelu

Jeśli z tej lekcji masz zrobić tylko jedną rzecz, to zapoznaj się z filmem na temat LangFuse i uruchom przykład o tej samej nazwie (langfuse) na swoim komputerze.